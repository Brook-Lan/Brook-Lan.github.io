---
layout: post
title: 李航《统计学习方法》学习笔记(6)--逻辑斯蒂回归
categories: 阅读笔记
tags: 机器学习
---

* content
{:toc}
逻辑斯蒂回归（logistic regression）也称对数几率回归。对于每个输入x，其y的条件概率是个连续变量，通过设定一个阈值来判断x所数的分类，这个阈值通常设为0.5。


## 1、 二项逻辑斯蒂回归模型

二项逻辑斯蒂回归是一种分类模型，随机变量X取值为实数，随机变量Y取值为1或0，有如下条件概率分布：

$$P(Y=1|x)=\frac{exp(w \cdot x + b)}{1 + exp(w \cdot x+b)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(w \cdot x+b)}$$

这里，x为权值，b为偏置，w ⋅ x 为w与x的内积。为了方便，可以将b并入权值，即 $$w = (w^{(1)}, w^{(2)},...,w^{(n)}, b)^T$$，$$x=(x^{(1)}, x^{(2)}, ...,x^{(n)}, 1)$$。于是，上面的式子变成：

$$P(Y=1|x)=\frac{exp(w \cdot x)}{1 + exp(w \cdot x)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(w \cdot x)}$$

逻辑斯蒂回归比较两个条件概率值的大小，将实例x 分到概率较大的那一类。

事件的几率是该事件发生的概率与该事件不发生的概率 1-p 的比值，该事件的对数几率为：

$$logit(p)=\log \frac{p}{1-p}$$

对逻辑斯蒂回归而言：

$$log \frac{P(Y=1|x)}{1-P(X=1|x)}=w \cdot x$$

这就是说，在逻辑斯蒂回归模型中，输出Y=1的对数几率是输入x的线性模型。

实际上是用线性回归模型的预测结果取逼近真实标记的对数几率，因此，对应的模型称为“对数几率回归”（logistic regression）。虽然它的名字是“回归”，但实际上一种分类学习方法。这种方法有很多有点，例如它是直接对分类的可能性建模，无需事先假定数据分布，这样就避免了假设分布不准确所带来的问题；它不仅预测出“类别”，而是可以得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；此外，它是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可以直接用于求取最优解。

## 2、参数估计

极大似然法估计模型参数

设： 

$$P(Y=1|x)=\pi(x)$$

$$P(Y=0|x)=1-\pi(x)$$

似然函数为:

$$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$
\begin{align}
L(w)&=\sum_{i=1}^N[y_i\log \pi(x_i) + (1-y_i)\log (1-\pi(x_i))]\\
&=\sum_{i=1}^N[y_i \log \frac{\pi (x_i)}{1-\pi (x_i)} + \log (1-\pi(x_i))]\\
&=\sum_{i=1}^N[y_i(w \cdot x_i) - \log (1+exp(w \cdot x_i))]
\end{align}
$$

对L(w) 求极大值（通常采用梯度下降及拟牛顿法），得到w的估计值$$\hat{w}$$，学到的逻辑斯蒂回归模型为：

$$P(Y=1|x)=\frac{exp(\hat{w} \cdot x)}{1 + exp(\hat{w} \cdot x)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(\hat{w} \cdot x)}$$



## 3、要点总结

- 简单介绍一下该算法：逻辑回归是在数据服从伯努利分布的假设下，通过极大似然法的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。

- 假设条件：

  - 数据服从伯努利分布
  - 样本为正的概率p为一个sigmoid函数

- 损失函数：极大似然函数（取对数）

- 参数求解方法：梯度下降法

- 优缺点：

  - 优点：1. 形式简单，模型的可解释性好（特征权值表示了该特征对结果的影响）; 2. 模型效果不错(baseline)，在工程上是可以接受的；3. 训练速度较快；4.方便调整输出结果（通过调整阈值的方式）
  - 缺点：1. 准确率欠佳（因为形式简单，现实中的数据非常复杂）；2. 很难处理数据不平衡的问题；3. 无法自动进行特征筛选；

- 与线性回归的区别：线性回归的损失函数为平方损失函数，逻辑斯蒂回归是对数似然函数

  