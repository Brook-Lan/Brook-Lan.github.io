---
layout: post
title: 李航《统计学习方法》学习笔记(6)--逻辑斯蒂回归
categories: 读书笔记
tags: 机器学习
---

* content
{:toc}
逻辑斯蒂回归（logistic regression）也称对数几率回归


### 1.1  二项逻辑斯蒂回归模型

二项逻辑斯蒂回归是一种分类模型，随机变量X取值为实数，随机变量Y取值为1或0，有如下条件概率分布：

$$P(Y=1|x)=\frac{exp(w \cdot x + b)}{1 + exp(w \cdot x+b)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(w \cdot x+b)}$$

这里，x为权值，b为偏置，w ⋅ x 为w与x的内积。为了方便，可以将b并入权值，即 $$w = (w^{(1)}, w^{(2)},...,w^{(n)}, b)^T$$，$$x=(x^{(1)}, x^{(2)}, ...,x^{(n)}, 1)$$。于是，上面的式子变成：

$$P(Y=1|x)=\frac{exp(w \cdot x)}{1 + exp(w \cdot x)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(w \cdot x)}$$

逻辑斯蒂回归比较两个条件概率值的大小，将实例x 分到概率较大的那一类。

事件的几率是该事件发生的概率与该事件不发生的概率 1-p 的比值，该事件的对数几率为：

$$logit(p)=\log \frac{p}{1-p}$$

对逻辑斯蒂回归而言：

$$log \frac{P(Y=1|x)}{1-P(X=1|x)}=w \cdot x$$

这就是说，在逻辑斯蒂回归模型中，输出Y=1的对数几率是输入x的线性模型。

实际上是用线性回归模型的预测结果取逼近真实标记的对数几率，因此，对应的模型称为“对数几率回归”（logistic regression）。虽然它的名字是“回归”，但实际上一种分类学习方法。这种方法有很多有点，例如它是直接对分类的可能性建模，无需事先假定数据分布，这样就避免了假设分布不准确所带来的问题；它不仅预测出“类别”，而是可以得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；此外，它是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可以直接用于求取最优解。

### 1.2 参数估计

极大似然法估计模型参数

设： 

$$P(Y=1|x)=\pi(x)$$

$$P(Y=0|x)=1-\pi(x)$$

似然函数为:

$$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$
\begin{align}
L(w)&=\sum_{i=1}^N[y_i\log \pi(x_i) + (1-y_i)\log (1-\pi(x_i))]\\
&=\sum_{i=1}^N[y_i \log \frac{\pi (x_i)}{1-\pi (x_i)} + \log (1-\pi(x_i))]\\
&=\sum_{i=1}^N[y_i(w \cdot x_i) - \log (1+exp(w \cdot x_i))]
\end{align}
$$

对L(w) 求极大值（通常采用梯度下降及拟牛顿法），得到w的估计值$$\hat{w}$$，学到的逻辑斯蒂回归模型为：

$$P(Y=1|x)=\frac{exp(\hat{w} \cdot x)}{1 + exp(\hat{w} \cdot x)}$$

$$P(Y=0|x)=\frac{1}{1 + exp(\hat{w} \cdot x)}$$

