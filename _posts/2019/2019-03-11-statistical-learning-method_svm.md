---
layout: post
title: 李航《统计学习方法》学习笔记(8)--支持向量机
categories: 读书笔记
tags: 机器学习
---
* content
{:toc}

- 基本模型是定义在特征空间上的间/隔最大的线性分类器，**间隔最大**使它有别于感知机。
- 核技巧使它成为实质上的非线性分类器。
- 学习策略是**间隔最大化**，形式化为求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。
- 线性可分支持向量机（数据线性可分时，由硬间隔最大化学习）、线性支持向量机（数据近似可分时，通过软间隔最大化学习）、非线性支持向量机（数据线性不可分时，通过核技巧及软间隔最大化学习）。
- 核函数表示**将输入从输入空间映射到特征空间得到的特征向量之间的内积**，等价于隐式地在高维特征空间中学习线性支持向量机。



## 一、线性可分支持向量机与硬间隔最大化

### 1.1 线性可分支持向量机

当训练数据集线性可分时，存在无穷个分离超平面将两类数据分开。感知机里利用误分类（的函数间隔）最小的策略，求得超平面，不过这时的结有无穷个。线性可分支持向量机利用间隔最大化（距离分离超平面最近的点的几何间隔）求最优分离超平面，这时，**解是唯一的**。

**线性可分支持向量机**：给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为

$$w^* \cdot x + b^* = 0$$

以及相应的分类决策函数

$$f(x)=sign(w^* \cdot x + b^*)$$

称为线性可分支持向量机。

### 1.2 函数间隔与几何间隔

对于给定的训练数据集T，定义超平面 (w, b) 关于样本点$$(x_i,y_i)$$的函数间隔:

 $$\hat \gamma_i = y_i(w \cdot x_i + b)​$$

定义超平面 (w, b) 关于训练数据集T 的函数间隔为超平面(w,b) 关于T中所有样本点 $(x_i, y_i)$的函数间隔的最小值，即：

$$\hat \gamma = \min_{i=1,...,N} \hat \gamma_i​$$

几何间隔：

$$\gamma_i = y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})$$

定义超平面 (w, b) 关于训练数据集T 的几何间隔为超平面(w,b) 关于T中所有样本点 $(x_i, y_i)$的几何间隔的最小值，即：

$$\gamma = \min_{i=1,...,N} \gamma_i$$

几何间隔与函数间隔的关系：

$$\gamma_i = \frac{\hat \gamma_i}{||w||}$$

$$\gamma = \frac{\hat \gamma}{||w||}$$

对于函数间隔，可以表示分类预测的正确性及确信度，但在选择超平面上，只要等比例地改变w和b，超平面没变，而函数间隔却变化了。使用几何间隔可以避免这一情况。

### 1.3 间隔最大化

支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。

$$
\begin{align}
&\max_{w,b}\quad\gamma \\
&s.t.\quad y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}) \ge \gamma, \quad\quad i=1,2,...,N
\end{align}
$$

由函数间隔和几何间隔的关系，可将上的问题改写成

$$
\begin{align}
&\max_{w,b}\quad\frac{\hat \gamma}{||w||} \\
&s.t.\quad y_i(w \cdot x_i + b) \ge \hat \gamma, \quad\quad i=1,2,...,N
\end{align}
$$

函数间隔 $$\hat \gamma$$ 的取值并不影响最优化问题的解，对目标函数的优化也没有影响，也就是说它产产生一个等价的最优化问题，这样就可以取 $$\hat \gamma=1$$ ，将其带入上面的最优化问题，$$同时最大化 \frac{1}{||w||} 和最小化 \frac{1}{2}||w||^2是等价的​$$ ，于是可以得到下面的最优化问题
$$
\begin{align}
&\min_{w,b}\quad\quad \frac{1}{2} ||w||^2 \tag{1.1} \\
&s.t.\quad\quad y_i(w \cdot x_i + b) -1 \ge 0, \quad\quad i=1,2,...,N \tag{1.2}
\end{align}
$$



### 1.4 支持向量

在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量，支持向量是使如下约束条件成立的点

$$y_i(w\cdot x_i + b)-1=0$$

在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。



## 二、线性支持向量机与软间隔最大化

通常情况是，训练数据集中有一些特异点，将这些特异点除去后，剩下大部分的样本点组成的几何是线性可分的。线性不可分意味着某些样本点不能满足函数间隔大于等于１的约束条件，为了解决这个问题，可以对每个样本点引入一个松弛变量 $$\xi_i \ge 0$$，使函数间隔大于等于１，这样约束条件变为

$$
\begin{align}
&\min_{w,b,\xi}\quad\quad \frac{1}{2} ||w||^2 +C\sum_{i=1}^N\xi_i\tag{2.1} \\
&s.t.\quad\quad y_i(w \cdot x_i + b) \ge 1-\xi_i, \quad\quad i=1,2,...,N \tag{2.2} \\
&\quad\quad\quad\quad \xi_i \ge 0, \quad\quad i=1,2,...,N \tag{2.3}
\end{align}
$$

其中 C>=0　称为惩罚参数，C值越大，对误分类的惩罚增大，反之惩罚减小。



## 三、非线性支持向量机与核函数

### 3.1 核技巧

非线性问题往往不好求解，通常所采取的办法是进行一个非线性变换，将非线性问题变换为线性问题，通过求解变换后的线性问题的方法求解原来的非线性问题。

基本想法是通过一个非线性变换将输入空间对应于一个特征空间，使得再输入空间中的超曲面模型对应于特征空间的超平面模型。

核函数：设$$\chi​$$是输入空间，$$H​$$为特征空间，如果存在一个从$$\chi​$$到$$H​$$的映射：

$$\phi(x):\chi \rightarrow H$$

使得对所有$$x,z \in \chi$$，函数K(x,z)满足条件:

$$K(x,z)=\phi(x)\cdot \phi(z)​$$

则称K(x,z)为核函数，$$\phi(x)$$为映射函数，$$\phi(x)\cdot \phi(z)$$为$$\phi(x)$$和$$\phi(z)$$的内积。

核技巧的想法是，在学习与预测中只定义核函数K(x,z)，而不显示地定义映射函数$$\phi$$。通常直接计算K(x,z)比较容易。

在线性支持向量机的对偶问题中，无论是目标函数还是决策函数，都只涉及输入实例之间的内积

### 3.2 正定核

通常所说的核函数就是正定核函数

### 3.3 常用核函数

多项式核函数:

$$K(x,z)=(x\cdot z + 1)^p$$

高斯核函数：

$$K(x,z)=exp(-\frac{||x-z||^2}{2\sigma ^2})$$

字符串核函数：

略

### 3.4 非线性支持向量分类机

从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划，学习得到分类决策函数

$$f(x)=sign(\sum_{i=1}^{N}a_i^*y_iK(x,x_i)+b^*)$$

称为非线性支持向量机，K(x,z)是正定核函数。