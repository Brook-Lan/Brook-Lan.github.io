---
layout: post
title: 李航《统计学习方法》学习笔记(10)--隐马尔可夫模型
categories: 读书笔记
tags: 机器学习
---
* content
{:toc}
## 一、隐马尔可夫模型

<span style="border-bottom:2px dashed red;">HMM是关于时序的概率模型，描述由一个**隐藏的马尔可夫链**生成不可观测的**状态随机序列**，再由各个状态序列生成一个观测而产生**观测随机序列**的过程。</span>

隐马尔可夫模型由初始状态概率向量$$\pi$$、状态转移概率矩阵A和观测概率矩阵B确定。状态转移概率矩阵A与初始状态概率向量$$\pi$$确定了隐藏的马尔科夫链，生成不可观测的状态序列，观测概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。

因此，隐马尔可夫模型$$\lambda$$可以用三元符号表示：

$$\lambda=(A,B,\pi)​$$

> 设Q是所有可能的状态集合(N个)，V是所有可能的观测的集合(M个)，I是长度为T的状态序列，O是对应的观测序列。
>
> $$Q=\{q_1,q_2,...,q_N\},\quad\quad\quad V=\{v_1,v_2,...,v_M\}$$
>
> $$I=(i_1,i_2,...,i_T),\quad\quad\quad\quad O=(o_1,o_2,...,o_T)$$
>
> 【状态转移概率矩阵A】:
>
> $$A=[a_{ij}]_{N\times N}$$
>
> 其中，
>
> $$a_{ij}=P(i_{t+1}=q_j | i_t=q_i),\quad\quad\quad i=1,2,...,N;\quad\quad j=1,2,...,N$$
>
> 表示由状态$$q_i$$条件下转移到状态$$q_j$$的概率。
>
> 【观测概率矩阵B】:
>
> $$B=[b_j(k)]_{N\times M}​$$
>
> 其中，
>
> $$b_j(k)=P(o_t=v_k|i_t=q_j),\quad\quad\quad k=1,2,...,M;\quad\quad j=1,2,...,N$$
>
> 表示由状态$$q_j$$条件下生成观测$$$v_k$的概率。
>
> 【初始状态概率向量】
>
> $$\pi = (\pi_i)$$
>
> 其中，
>
> $$\pi_i=P(i_1=q_i), \quad\quad\quad\quad i=1,2,...,N$$
>
> 是时刻t=1是处于状态$$q_i$$的概率。



由定义可知，隐马尔可夫模型做了两个基本假设:

1. 齐次马尔可夫性假设。即任意时刻t的状态只依赖其前一时刻 t-1 的状态，与其他时刻的状态和观测无关。
2. 观测独立性假设。即任意时刻t的观测只依赖于该时刻的马尔可夫链的状态，与其他时刻的观测和状态无关。

隐马尔可夫模型可以用于标注，这时状态对应标记。标注问题是给定观测序列预测其对应的标记序列。

隐马尔可夫模型有3个基本问题:

1. 概率计算。给定模型$$\lambda=(A,B,\pi)$$和观测序列$$O=(o_1,o_2,...,o_T)$$，计算 $$P(O｜ \lambda)$$
2. 学习问题。已知观测序列$$O=(o_1,o_2,...,o_T)$$，估计模型$$\lambda=(A,B,\pi)$$参数，使得$$P(O｜\lambda)$$最大。
3. 预测问题。给定模型与观测序列，求最优可能的状态序列。



## 二、概率计算算法

### 2.1 前向算法

详见原书 p175 ~ p176

### 2.2 后向算法

详见原书 p178 ~ p179

## 三、学习算法

### 3.1 监督学习算法

训练数据包含观测序列及对应的状态序列，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。

### 3.2　Baum-Welch算法

人工标注训练数据往往代价很高，有时就会利用非监督学习的方法。

算法详见原书 p181 ~ p183

## 四、预测算法

### 4.1 近似算法

近似算法的想法是，在每个时刻t选择在该时刻最有可能出现的状态$$i_t^*$$，从而得到一个状态序列$$I^*=(i_1^*,i_2^*,...,i_T^*)$$，将它作为预测的结果。

在时刻t处于状态$$q_i$$的概率$$\gamma_t(i)$$为

$$\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}$$

t时刻最有可能的状态是

$$i_t^*=\arg\max_{1\le i\le N}[\gamma_t(i)], \quad\quad\quad t=1,2,...,T$$

近似算法的优点是计算简单，缺点是不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。

### 4.2 维特比算法

维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大的路径(最优路径)。根据动态规划原理，我们只需从t=1开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直到时刻T。t=T时的最大概率即为最优路径的概率$$P^*​$$，最优路径的终结点$$i_T^*​$$也可同时得到。之后为了找出最优路径各个结点，从$$i_T^*​$$开始，由后向前逐步求得结点$$i_{T-1}^*,...,i_1^*​$$，得到最优路径$$I^*​$$

算法详见原书p184 ~ p185



## 五、代码实现

[python模块](https://github.com/hmmlearn/hmmlearn)