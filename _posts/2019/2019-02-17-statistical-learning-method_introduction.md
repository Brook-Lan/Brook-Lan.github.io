---
layout: post
title: 李航《统计学习方法》学习笔记(1)--概论
categories: 读书笔记
tags: 机器学习
---

* content
{:toc}


## 一、统计学习

统计学习关于数据的基本假设是**同类数据具有一定的统计规律。**



## 二、监督学习

### 2.1 监督学习方法

从给定的、有限的、用于学习的训练数据集合出发，假设数据是**独立同分布**产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测。

### 2.2 基本概念

- 输入空间、特征空间与输出空间

  a) 将模型的输入与输出所有可能取值的集合分别称为**输入空间**和**输出空间**。

  b) 每个具体的输入是一个实例，由特征空间表示。所有特征向量存在的空间称为**特征空间**。特征空间的每一维对应一个特征。有时输入空间与特征空间为相同的空间；有时输入空间和特征空间为不同的空间，将实例从输入空间映射到特征空间。模型实际上都是定义在特征空间上的。

- 联合概率分布

  监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数或分布密度函数。统计学习假设数据存在一定的统计规律，**X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设**。

- 假设空间

  由输入空间到输出空间的映射的集合，称为**假设空间**。监督学习的目的就是在这个假设空间中找到一个最好的映射，即模型。

  ​



## 三、统计学习三要素

方法 = 模型 + 策略 + 算法

### 3.1 模型

在监督学习过程中，模型是所要学习的条件概率分布<sup>【备注】</sup>或决策函数。

【备注】：本章后面介绍生成模型时谈到**联合概率分布**，故这边没谈到联合概率分布应该是作者遗漏了。

### 3.2 策略

学习或选择最优模型所遵从的准则。

#### 损失函数与风险函数

- 损失函数：模型的输出与真实的y值可能不一致，用一个**损失函数**或**代价函数**来度量预测错误的程度。损失函数的值越小，模型就越好。常用的损失函数有：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数。
- 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。

#### 经验风险最小化与结构风险最小化

**经验风险最小化**的策略认为经验风险最小的模型是最优模型。当样本容量足够大时，经验风险最小化能保证有很好的学习效果，比如极大似然估计就是经验风险最小化的一个例子。

**结构风险最小化**的策略认为结构风险最小的模型是最优模型。但当样本容量很小时，经验风险最小化学习策略容易产生过拟合现象。**结构风险最小化**是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项(J(f))。模型f越复杂，复杂度J(f)就越大。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。

### 3.3 算法

统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。统计学习的最优化问题通常都你没有显式的解析解，这时就需要用数值计算的方法求解，如梯度下降法。



## 四、模型选择

### 4.1 正则化

模型选择的典型方法是正则化。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项。**正则化项一般是模型复杂度的单调递增函数**。模型越复杂，正则化值就越大。常用的正则化项有L1范数、L2范数。

经验风险较小的模型可能较复杂，这时模型复杂度会较大，正则化的作用是选择经验风险与模型复杂度同时较小的模型。正则化符合奥卡姆剃刀原理，在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的模型才是最好的模型。

### 4.2 交叉验证

另一种常用的模型选择方法是交叉验证。

在许多实际运用中数据是不足的，为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选取。

- 简单交叉验证：随机将数据分成两份，一部分作为训练集，另一部分作为测试集（如70%训练，30%测试）
- s折交叉验证：随机将数据切分成s个互不相交的大小相同的子集，然后将s-1个子集的数据训练模型，余下的子集测试模型；将这一个过程对可能的s种选择重复进行，最后选出s次评测中平均误差最小的模型
- 留一交叉验证：s折交叉验证的特殊情形是s等于数据集的大小。


**补充：**

交叉验证的用途：

- 选择超参数：sklearn中的网格搜索法GridSearchCV函数中有个cv参数，以此用交叉验证来获取最优超参数
- 评估模型的性能：sklearn中的cross_val_score函数,可用来以交叉验证方法来评估模型




## 五、生成模型与判别模型

生成方法由数据学习**联合概率分布**P(X,Y)，然后求出条件概论分布P(Y\|X)作为预测的模型。之所以成为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。典型的生成模型有：朴素贝叶斯法和因马尔科夫模型等。

判别方法由数据直接学习**决策函数**或者**条件概率分布**作为预测模型即判别模型（直接得到判断边界）。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。典型的判别模型有：k近邻发、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升法和条件随机场等。

生成方法的特点：

- 生成方法可以还原出联合概率分布P(X,Y)。
- 学习收敛速度更快。
- 当存在隐变量时，仍可以用生成方法学习，而判别方法不能。

判别方法特点：

- 直接学习的是条件概率P(Y\|X)或决策函数，直接面对预测，往往学习的准确率更高。
- 由于直接学习条件概率或决策函数，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。



