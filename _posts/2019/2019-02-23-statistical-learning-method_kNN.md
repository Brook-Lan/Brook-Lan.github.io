---
layout: post
title: 李航《统计学习方法》学习笔记(3)--k近邻法
categories: 阅读笔记
tags: 机器学习
---

* content
{:toc}
k近邻法分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过**多数表决**等方式进行预测。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。k近邻法不具有显式的学习过程，k值选择、距离度量及分类决策规则是k近邻法的三个基本要素。



## 一、距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻法中采用**欧式距离**：

L<sub>2</sub>(x<sub>i</sub>,y<sub>j</sub>) =  (∑\|x<sub>i</sub><sup>(l)</sup> - y<sub>j</sub><sup>(l)</sup>\|<sup>2</sup>)<sup>1/2</sup> 

其中l=1,2,...,n(n为实例的维度个数)

也可以使用其他的距离：L<sub>p</sub>距离、Minkowski距离等，不同的距离度量所确定的最近邻点是不一样的。



## 二、k值选择

k值较小，就就相当于用较小的邻域中的训练实例进行预测，预测结果会对近邻的实例点非常敏感。“学习”的近似误差会减小，估计误差会增大。换句话说，k值减小意味着整体模型变得复杂，容易发生过拟合。

k值较大，相当于用较大的邻域中的训练实例进行预测，与输入实例较远的训练实例点也会对预测起作用。“学习”的近似误差会增大，估计误差会减小。换句话说，k值减小意味着整体模型变得简单。

在应用中，一般选取一个较小的k值，通常采用交叉验证法来选取最优k值。



## 三、分类决策规则

通常采用多数表决，等价于经验风险最小化。



## 四、kd树

k近邻法的实现需要考虑如何快速搜索k个最近的点。kd树是一种便于对k维空间中的数据进行快速检索的数据的结构。kd树是二叉树，表示对k维空间的一个划分，其每个节点对应于k维空间划分中的一个超矩形区域。

更详细的kd树构建与搜索请翻阅原书或在网上查找相关资料。