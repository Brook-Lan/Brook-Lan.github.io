---
layout: post
title: 李航《统计学习方法》学习笔记(5)--决策树
categories: 阅读笔记
tags: 机器学习
---

* content
{:toc}


决策树可以被认为是 if-then 的规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。决策树的学习通常包括3个步骤：特征选择、决策树的生成和决策树的剪枝。



## 一、决策树模型与学习

### 1.1 决策树模型

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种：(1)内部结点--表示一个特征或属性；(2)叶结点--表示一个类

用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。

### 1.2 决策树与if-then规则

可以将决策树看成一个if-then 规则的集合：由决策树的根节点到叶节点的每一条路径构建一条规则；路径上的内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论。

### 1.3 决策树与条件概率分布

决策树还可以表示给定特征条件下类的条件概率分布：将特征空间划分为互不相交的单元区域，并在每个单元定义一个类的概率分布就构成一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。各叶结点（单元）上的条件概率往往偏向某一个类，决策树分类时将该结点的实例强分到条件概率大的那一类去。

### 1.4 决策树学习

决策树学习本质上是从训练数据集中归纳出一组分类规则。从另一个角度看，决策树学习是由训练数据集估计条件概率分布模型。

决策树学习的损失函数通常是**正则化的极大似然函数**。

决策树学习算法通常是一个递归选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策数的构建。

> 开始，构建根结点，将所有训练数据都放在根节点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有明确的类，这就生成了一棵决策树。

为防止过拟合，我们还需要对生成的决策树进行剪枝，使决策树变得简单，具有更好的泛化能力。

可以看出，决策树学习算法包含**特征选择、决策树的生成与决策树的剪枝**。



## 二、特征选择

### 2.1 特征选择问题

特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是**信息增益**或**信息增益比**。

如果利用一个特征进行分类的结果与随机分类的结果没有很大的差别，则称这个特征是没有分类能力的。

### 2.2 信息增益

**熵**是表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大。

设X是一个取有限个值的离散随机变量，其概率分布为：

$$P(X=x_i) = p_i,    i=1,2,...,n$$

则其熵的定义为：

$$H(X)=-\sum_{i=1}^{n}p_i\log p_i$$

由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可以将X的熵记作 H(p)。

设随机变量 (X, Y) 的联合概率分布为：

$$P(X=x_i, Y=y_j) = p_{ij},    i=1,2,...,n;  j=1,2,...,m$$

**条件熵**H(Y\|X) 表示在已知随机变量X的条件下随机变量Y 的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：

$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$

这里， $$p_i=P(X=x_i),    i=1,2,...,n$$

当熵和条件熵中的概率由数据估计（极大似然估计）得到时，所对应的熵和条件熵分别称为**经验熵**和**经验条件熵**。

**信息增益**表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

特征	A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件上H(D\|A)之差：

$$g(D,A) = H(D) - H(D|A)$$

一般地，熵 H(Y)与条件熵 H(Y\|X)之差称为**互信息**，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

**决策树学习应用信息增益准则选择特征，信息增益大的特征具有更强的分类能力。**

根据信息增益准则的特征选择方法是：对训练数据集(或子集) D，计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。

### 2.3 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择值较多的特征的问题。使用信息增益比可以对这一问题进行校正。

特征 A 对训练数据集 D 的信息增益比$$g_{_R}(D,A)$$定义为其信息增益 g(D,A)与训练数据集 D 关于特征 A 的值的熵$$H_A(D)$$之比，即：

$$g_{_R}(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中，

$$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$ 

n 是特征A的取值个数。



## 三、决策树的生成

### 3.1 ID3算法

采用信息增益准则选择特征，递归地构建决策树。ID3 相当于用极大似然法进行概率模型选择。

### 3.2 C4.5算法

用信息增益比来选择特征。

## 四、决策树的剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。

设树 T 的叶结点个数为\|T\|， t 是树 T的叶结点，该叶结点有$$N_t$$个样本点，其中k类的样本点有$$N_{tk}$$个，k=1,2,...,K，$$H_t(T)$$为结点t上经验熵， a >= 0为参数，则决策树的损失函数为：

$$C_a(T) = \sum_{t=1}^{|T|}N_tH_t(T) + a|T|$$

其中，经验熵为：

$$H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}$$

令：

$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log \frac{N_{tk}}{N_t}$$

则有:

$$C_a(T)=C(T) + a|T|$$

上式中， C(T)表示模型对训练数据的预测误差，\|T\| 表示模型的复杂度， 参数 a>=0 控制两者之间的影响，较大的 a 促使选择较简单的模型，反之使模型更复杂。

剪枝，就是当 a 确定时，选择损失函数最小的模型。

可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部模型，而决策树剪枝学习整体模型。

利用损失函数最小原则进行剪枝就是用了正则化的极大似然估计进行模型选择。

**剪枝算法：**

输入：生成算法产生的整个树T，参数 a;

输出：修剪后的子树 $${T_a}$$

(1) 计算每个结点的经验熵

(2) 递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为$$T_B$$ 与 $$T_A$$，其对应的损失函数分别是 $$C_a(T_B)$$和 $$C_a(T_A)$$，如果：

$$C_a(T_A) <= C_a(T_B)$$

则进行剪枝，即父结点变为新的叶结点

(3) 返回(2)，直至不能继续为止，得到损失函数最小的子树$$T_a$$





## 五、CART算法

分类与回归树（classification and regression tree, CART）模型既可以用来分类，也可以用来回归。

CART 假设决策树是**二叉树**，内部结点特征的取值为“是”和“否”，这样的决策树等价于递归地二分每个特征，将输入空间（特征空间）划分为有限个单元，并在这些单元上确定预测的概率分布（输入条件给定的条件下输出的条件概率分布）。

### 5.1 CART 生成

#### 5.1.1 回归树的生成

回归树用**平方误差最小化**准则进行特征选择。

一个回归树对应着输入空间（特征空间）的一个划分以及在划分的单元上的输出值。这个输出值为该单元里的所有输入实例对应的输出的平均值。

对输入空间进行划分的一个关键点是：寻找最优的切分特征和最优的切分点(切分点对应切分特征的一个取值)。

划分的大致的流程是：对于给定的训练数据集，遍历每一个特征维度j，在特征维度j上再遍历该维度上的每个取值s，得到候选划分点(j,s)，针对这个划分点计算被分割两个子区域的平方损失并求它们的和e。所有遍历完后，选择最小的e所对应的(j, s)作为最优的切分点，数据被分为两部分后，对每个部分再重复前面的划分步骤直到满足停止条件。

更多的细节参看原书68~69页，结合上面这段的表述应该可以会很好理解。文末也引用了篇别人的博客，可以看看。

#### 5.1.2 分类树的生成

分类树用**基尼指数**选择最优特征。

假设有K个类，样本点属于第k类的概率为$$p_k$$， 则概率分布的基尼指数为：

$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$

对给定样本集合D，其基尼指数为：

$$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$$

样本集合D根据特征A是否取某一个可能值a被分割成D1和D2，则在特征A的条件下，集合D的基尼指数定义为：

$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$$

基尼指数Gini(D)表述集合D的不确定性，与熵一样，基尼指数值越大，样本集合的不确定性也就越大。

分类数的生成树的生成类似，只是在候选切分点上计算的是基尼系数，最终选择基尼系数最小的切分点作为最优切分点。

### 5.2 CART 剪枝

略

## 六、参考阅读

[机器学习笔记十二：分类与回归树CART](https://blog.csdn.net/xierhacker/article/details/64439601)