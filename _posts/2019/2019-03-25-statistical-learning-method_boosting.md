---
layout: post
title: 李航《统计学习方法》学习笔记(９)--提升方法
categories: 阅读笔记
tags: 机器学习
---
* content
{:toc}
## 一、AdaBoost

### 1.1 提升方法

大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。

这样，对提升方法来说，有两个问题需要回答：

- 每一轮训练中如何改变数据的权值（分布）
- 如何将弱分类器组合成一个强分类器

AdaBoost的做法是：

- 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值；

- 采取加权多数表决的方法组合弱分类器，即加大误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。



### 1.2 AdaBoost算法


有训练数据$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$，其中$$y \in \{-1, 1\}$$

(1) 初始化训练数据的权值分布

$$D_1=(w_{1,1},...,w_{1,i},...,w_{1,N}),\quad\quad w_{1,i}=\frac{1}{N},\quad\quad i=1,2,...,N$$

(2)对m=1,2,...,M

(a)使用具有权值分布D_m的训练数据集学习，得到基本分类器$$G_m(x)$$

(b)计算$$G_m(x)$$在训练数据集上的误差率

$$e_m=P(G_m(x_i) \ne y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i) \ne y_i)$$

(c)计算$$G_m(x)​$$Ｄ的系数

$$\alpha_m = \frac{1}{2}\log \frac{1-e_m}{e_m}$$

(d)更新训练数据的权值分布

$$D_{m+1}=(w_{m+1,i},...,w_{m+1,i},...,w_{m+1,N})$$

$$w_{m+1,i}=\frac{w_{m,i}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\quad\quad i=1,2,...,N$$

其中$$Z_m$$是规范化因子

$$Z_m=\sum_{i=1}^N w_{m,i} \exp(-\alpha_my_iG_m(x_i))$$

(3)构建基本分类器的线性组合

$$f(x)=\sum_{m=1}^M \alpha_mG_m(x)​$$

得到最终分类器

$$G(x)=sign(f(x))=sign(\sum_{m=1}^M \alpha_mG_m(x))$$

> 【基本分类器】
>
> 从上面的算法可知，基本分类器需要对含有权重的训练数据进行训练，通过书本后面的adaboost例子和网上别人对adaboost的代码实现可知，adaboost里的基本分类器是深度为１的决策树，即**决策树桩**。
>
> 对于含有多个特征维度的训练数据而言，需要遍历每个特征，在每个特征上找合理的分界点（结合训练数据的权值，通过计算误差率来找最佳的分界点）,最后在各个特征维度的最佳分界点中选择最小误差率的维度的分界点作为决策树桩的最佳分界点。注意，不管训练数据有多少特征，最终决策树桩只会根据某一个特征的分解点来判断类别，同时这边没有采用信息增益来划分数据集，而是根据计算误差率（与训练数据权值挂钩）来选择最佳的分界点。



### 1.3 前向分布算法

AdaBoost算法可以认为是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法。

考虑加法模型

$$f(x)=\sum_{m=1}^M \beta_mb(x;\gamma_m)$$

其中$$b(x;\gamma_m)$$为基函数，$$\gamma_m$$为基函数的参数，$$\beta_m$$为基函数的系数。

学习加法模型成为损失函数最小化问题

$$\min_{\beta_m,\gamma_m} \sum_{i=1}^{N}L(y_i,\sum_{m=1}^M\beta_m b(x_i;\gamma_m)) \tag{1.3}$$

前向分步算法的想法是：对于加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近目标函数(1.3)，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：

$$\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))$$



## 二、提升树

### 2.1 提升树

以决策树为基函数的提升方法称为提升树。对分类问题，决策树是二叉分类树；对回归问题，分类树是二叉回归树。

提升树模型可以表示为决策树的加法模型：

$$f_M(x)=\sum_{m=1}^MT(x;\theta_m)$$

$$T(x,\theta_m)$$表示决策树；$$\theta_m$$为决策树的参数；M为决策树的个数。

提升树可以采用前向分步算法。首先确定初始提升树$$f_0(x)=0$$，第m步的模型是

$$f_m(x)=f_{m-1}(x)+T(x;\theta_m)​$$

通过经验风险最小化确定下一棵决策树的参数 $$\theta_m​$$

$$\hat\theta_m=\arg\min_{\theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\theta_m))$$



### 2.2 梯度提升

关键是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差近似值，拟合一个回归树。