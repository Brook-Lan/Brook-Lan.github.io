---
layout: post

title: 李航《统计学习方法》学习笔记(7)--最大熵模型

categories: 阅读笔记

tags: 机器学习

---

* content
{:toc}

## 一、最大熵原理

最大熵原理是概率模型学习的一个准则：学习概率模型时，在所有可能的概率模型中，熵最大的模型就是最好的模型。

直观地，最大熵原理认为，要选择的概率模型首先必要满足已有的事实（即约束条件），在没有多信息的情况下，那些不确定的部分都是“等可能的”。熵的最大化表示等可能性。



## 二、最大熵模型

将最大熵原理运用到分类得到最大熵模型。

假设分类模型是一个条件概率分布P(Y\|X)，表示给定的输入X以条件概率P(Y\|X)输出Y。给定训练数据集，学习的目标是用最大熵原理选择最好的分类模型。

### 2.1 模型的约束条件

根据给定的训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布，分别用$$\hat P(X,Y)$$和$$\hat P(X)$$表示：

$$\hat P(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}$$

$$\hat P(X=x)=\frac{v(X=x)}{N}$$

其中，v(X=x, Y=y)表示训练数据中样本(x, y)出现的频数，v(X=x)表示训练数据中输入x出现的频数，N表示样本容量。

特征函数 f(x,y) 描述输入x和输出y之间的某一个定义：

$$f(x,y) = \begin{cases}
1,  \quad x与y满足某一事实  \\ 
0,  \quad 否则
\end{cases}
\tag{2.1}
$$

> 个人经验：早期学习这一章的时候，特征函数这一"新事物"（前面的其他分类模型都没涉及到这个概念）让我困惑了好长一段时间：x和y满足的某一**事实**是什么事实？在代码实现过程中如何定义它们？本章及在网上搜到的资料都解释到式子2.1这一层就点到为止，没有更深入地阐述了。继续往后看就会发现，在条件随机场那一章里也出现了特征函数。后面查阅了github上别人对最大熵模型的代码实现，他们的做法是这样：
>
> 假设有训练数据$$T=\{X,Y\}$$，令:
>
> $$\begin{align}
>
>  j&=1,2,...,J,\quad\quad J为X的特征维度 \\
>
>  i&=1,2,...,n_j,\quad\quad n_j为X在j特征上所有可能取值的个数 \
>
> x^{j,i}表示X在第j个特征的第i个可能取值
>
> \end{align}$$
>
> 则特征函数的形式为:
>
> $$f_{i,j}(x,y)=I(x^{j,i}, y),\quad\quad I为指示函数$$
>
> 总共有$$\sum_{j=1}^Jn_j​$$个特征函数，即对于一个实例x，其特征函数只涉及x的一个维度与对应的y。这种处理可能是为了计算方便，因为显然可以类推特征函数可以联合x的多个特征维度，只是那样复杂度就可能非常高了。

特征函数关于经验分布$$\hat P(X,Y)$$的期望值记为$$E_{\hat P}(f)$$，

$$E_{\hat P}(f) = \sum_{x,y}\hat P(x,y)f(x,y)$$

特征函数f(x,y)关于模型P(Y\|X)与经验分布$$\hat P(X)$$的期望值记为$$E_P(f)$$，

$$E_P(f)=\sum_{x,y}\hat P(x)P(y|x)f(x,y)$$

如果模型能够获取训练数据中的信息，那么可以假设两个期望值相等，即：

$$E_P(f) =E_{\hat P}(f)$$

或

$$\sum_{x,y}\hat P(x,y)f(x,y) = \sum_{x,y}\hat P(x)P(y|x)f(x,y) $$

我们将上式作为模型的约束条件，假如有n个特征函数$$f_i(x,y),\quad i=1,2,...,n$$，那么就有n个约束条件。



### 2.2 最大熵模型

假设满足所有约束条件的模型结合为

$$C \equiv  \{P \in \rho |E_P(f_i)=E_{\hat P}(f_i), \quad i=1,2,...n\}$$

定义在条件概率分布P(Y\|X)的条件熵为

$$H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)$$

则模型集合C中条件熵H(P)最大的模型称为最大熵模型。



### 2.3 最大熵模型的学习

最大熵模型的学习等价于约束最优化问题：

$$\begin{align}
&\max_{P \in C}\quad\quad H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)  \\
& s.t\quad\quad  E_p(f_i) = E_{\hat P}(f_i), \quad\quad i=1,2,...,n  \\
& \sum_yP(y|x)=1
\end{align}$$

由此可推导出（推导过程见原书83~85页）:

$$P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y)) \tag{2.2}$$

其中，

$$Z_w(x)=\sum_y\exp(\sum_{i=1}^nw_if_i(x,y)) \tag{2.3}$$

$$Z_w(x)$$称为规范化因子；$$f_i(x,y)$$是特征函数；$$w_i$$是特征的权值。式2.2和2.3表示的模型就是最大熵模型，ｗ是模型的参数。

模型参数w可以采用迭代尺度法、梯度下降法、牛顿法或拟牛顿法（详见原书88~92页）

## 三、附

大神们对最大熵模型的实现

[python版](https://github.com/WenDesi/lihang_book_algorithm/blob/master/maxENT/maxENT.py)

[java版](https://github.com/hankcs/maxent_iis)

